// scalastyle:off println

import java.util

import kafka.serializer.StringDecoder
import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.{AnalysisException, Row, SQLContext}
import org.apache.spark.streaming._
import org.apache.spark.streaming.kafka._
import org.apache.spark.{SparkConf, SparkContext}

import scala.util.control.Breaks.{break, breakable}
import scala.util.parsing.json.JSONObject


/**
  * [ahinchliff@hdp-data-02 ~]$ /usr/hdp/current/spark2-client/bin/spark-submit
  * --master yarn
  * --class "SparkKafkaDnsMalwareDomains"
  * uber-spark-scala-kafka-dns-malware-domain-0.1-SNAPSHOT.jar
  * kafka-broker:6667 dns /input/malwaredomainlist/malwaredomainlist_20180122.csv
  *
  */
object SparkKafkaDnsMalwareDomains {
  def main(args: Array[String]) {
    if (args.length < 3) {
      System.err.println(
        s"""
           |Usage: spark-submit ... <brokers> <topics> <hdfs path to malware domain list>
           |  <brokers> is a list of one or more Kafka brokers
           |  <topics> is a list of one or more kafka topics to consume from
           |
        """.stripMargin)
      System.exit(1)
    }

    val Array(brokers, topics, mdlPath) = args

    // Create context with 2 second batch interval
    val sparkConf = new SparkConf()
    val sc = new SparkContext(sparkConf)
    val ssc = new StreamingContext(sc, Seconds(2))
    val sqlContext = new SQLContext(sc)
    import sqlContext.implicits._ // for `toDF` and $""


    // load the Malware Domain List from CSV.
    // it's pretty small (300KB), it should easily fit into memory
    // http://www.malwaredomainlist.com/forums/index.php?topic=3270.0
    // file contains these fields in this order:
    // date,domain,ip,reverse,description,registrant,asn,inactive,country
    var mdlDF = sqlContext.read
      .format("com.databricks.spark.csv")
      .option("header", "false")
      .load(mdlPath)
      .cache()
    //mdlDF.show(2)

    // need to attempt to clean up Domains in MDL
    val domainPattern = "((?!-)[A-Za-z0-9-]{1,63}(?<!-)\\.)+[A-Za-z]{2,6}"
    mdlDF = mdlDF.withColumn("domain",
      when(col("_c1") === "-",
        regexp_extract(col("_c3"), domainPattern, 0))
        .otherwise(regexp_extract(col("_c1"), domainPattern, 0)))
    //mdlDF.printSchema()
    mdlDF.filter(col("domain").isNotNull)
    //mdlDF.show(20, false)

    // Create direct kafka stream with brokers and topics
    val topicsSet = topics.split(",").toSet
    val kafkaParams = Map[String, String]("metadata.broker.list" -> brokers)
    val dnsMessages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](
      ssc, kafkaParams, topicsSet)

    //"message":"Jan 22 15:26:31 dnsmasq[9958]: reply houzz.map.fastly.net is 151.101.192.93"
    // handle different message types (query, reply, cache, config, forwarded, /etc/hosts, etc?)
    val dnsPattern = ".*?:\\s+([/\\w\\[\\]]*)\\s+(.*?)\\s+\\w*(.*)"

    dnsMessages.foreachRDD(
      rdd => {
        if (!rdd.partitions.isEmpty) {
          breakable {
            var dnsDF = sqlContext.read.json(rdd.map(_._2))
            //dnsDF.printSchema()

            // not all DNS messages have the "message" key?
            // org.apache.spark.sql.AnalysisException: cannot resolve '`message`' given input columns: [];;
            try {
              dnsDF = dnsDF.withColumn("queryType", regexp_extract($"message", dnsPattern, 1))
              dnsDF = dnsDF.withColumn("domain", regexp_extract($"message", dnsPattern, 2))
              dnsDF = dnsDF.withColumn("address", regexp_extract($"message", dnsPattern, 3))
              //dnsDF.printSchema()
              //dnsDF.show(false)
            } catch {
              case e: AnalysisException => {
                // we want to continue processing the messages that we can process
                println("WARNING: Could not extract domain from message: " + e)
                dnsDF.show(false)
                break
              }
            }

            // join the datasets to find matching results
            var malwareDNS = dnsDF.join(mdlDF, dnsDF("domain") === mdlDF("domain"))
            //malwareDNS.printSchema()
            //malwareDNS.show(false)

            // JSONObject cannot handle null fields
            malwareDNS = malwareDNS.withColumn("_c9", when(col("_c9") isNull, ""))

            // write each result to Kafka
            // in general this is an inefficient way to send messages to Kafka, but malware results should be rare.
            malwareDNS.foreachPartition(partitionOfRecords => {
              val props = new util.HashMap[String, Object]()
              props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers)
              props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
                "org.apache.kafka.common.serialization.StringSerializer")
              props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
                "org.apache.kafka.common.serialization.StringSerializer")
              val producer = new KafkaProducer[String, String](props)

              partitionOfRecords.foreach {
                case row: Row => {
                  val rowMap = row.getValuesMap(row.schema.fieldNames)

                  try {
                    val rowString = JSONObject(rowMap).toString()

                    val message = new ProducerRecord[String, String]("dnsMalware", null, rowString)
                    producer.send(message)
                  } catch {
                    case e: NullPointerException => {
                      println("WARNING: NullPointer exception " + e + " from " + rowMap)
                    }
                  }
                }
              }
            })

          }

        }

      }
    )

    // Start the computation
    ssc.start()
    ssc.awaitTermination()
  }
}